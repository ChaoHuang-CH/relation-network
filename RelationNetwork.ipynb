{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 용어\n",
    "context: question에 대한 hint sentence의 묶음 <br>\n",
    "sentence: context를 이루고 있는 문장 하나 <br>\n",
    "question: 질문 (문장 하나)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import slim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# question, answer, context, label\n",
    "with open('./babi_preprocessd/train_dataset_masked.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open('./babi_preprocessd/val_dataset_masked.pkl', 'rb') as f:\n",
    "    val = pickle.load(f)\n",
    "with open('./babi_preprocessd/test_dataset_masked.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./babi_preprocessd/c_word_set.pkl', 'rb') as f:\n",
    "    c_word_set = pickle.load(f)\n",
    "with open('./babi_preprocessd/q_word_set.pkl', 'rb') as f:\n",
    "    q_word_set = pickle.load(f)\n",
    "with open('./babi_preprocessd/a_word_set.pkl', 'rb') as f:\n",
    "    a_word_set = pickle.load(f)\n",
    "with open('./babi_preprocessd/cqa_word_set.pkl', 'rb') as f:\n",
    "    cqa_word_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[train_q, train_a, train_c, train_l, train_c_real_len, train_q_real_len] = train\n",
    "[val_q, val_a, val_c, val_l, val_c_real_len, val_q_real_len] = val\n",
    "[test_q, test_a, test_c, test_l, test_c_real_len, test_q_real_len] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_max_len = 20\n",
    "s_max_len = 12\n",
    "q_max_len = 12\n",
    "mask_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 2e-4\n",
    "batch_size = 64\n",
    "iter_time = 6\n",
    "display_step = 100\n",
    "seed = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* context words: 124 <br>\n",
    "* question words: 88 <br>\n",
    "* answer words: 41 <br>\n",
    "* all words: 159 <br>\n",
    "* s,q max len: 12 <br>\n",
    "* c max len: 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM for context\n",
    "* 32 unit LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM for question\n",
    "* 32 unit LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model parameter\n",
    "s_input_step = s_max_len\n",
    "s_hidden = 32\n",
    "# ---\n",
    "q_input_step = q_max_len\n",
    "q_hidden = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# embed matrix\n",
    "c_word_embed = 32\n",
    "c_vocab_size = len(c_word_set)+1 # consider masking\n",
    "c_word_embed_matrix = tf.Variable(tf.random_uniform(shape=[c_vocab_size, c_word_embed], minval=-1, maxval=1, seed= seed))\n",
    "# ---\n",
    "q_word_embed = 32\n",
    "q_vocab_size = len(q_word_set)+1 # consider masking\n",
    "q_word_embed_matrix = tf.Variable(tf.random_uniform(shape=[q_vocab_size, q_word_embed], minval=-1, maxval=1, seed=seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input, output\n",
    "c = tf.placeholder(dtype=tf.int32, shape=[batch_size, c_max_len, s_max_len])\n",
    "c_real_len = tf.placeholder(dtype=tf.int32, shape=[batch_size, c_max_len])\n",
    "s = tf.placeholder(dtype=tf.int32, shape=[batch_size, s_max_len]) \n",
    "q = tf.placeholder(dtype=tf.int32, shape=[batch_size, q_max_len])\n",
    "q_real_len = tf.placeholder(dtype=tf.int32, shape=[batch_size])\n",
    "l = tf.placeholder(dtype=tf.float32, shape=[batch_size, c_max_len, c_max_len])\n",
    "a = tf.placeholder(dtype=tf.float32, shape=[batch_size, len(cqa_word_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contextLSTM(c, l, c_real_len, reuse=True, scope= \"contextLSTM\"):\n",
    "    \n",
    "    def sentenceLSTM(s, s_real_len, s_hidden=s_hidden, s_max_len= s_max_len, reuse=reuse, scope= \"sentenceLSTM\"):\n",
    "        \"\"\"\n",
    "        embedding sentence\n",
    "\n",
    "        Arguments\n",
    "            s: sentence (word index list), shape = [batch_size, 12]\n",
    "            s_real_len: length of the sentence before zero padding, int32\n",
    "\n",
    "        Returns\n",
    "            embedded_s: embedded sentence, shape = [batch_size, 32]\n",
    "        \"\"\"\n",
    "        embedded_sent_word = tf.nn.embedding_lookup(c_word_embed_matrix, s)\n",
    "        s_input = tf.unstack(embedded_sent_word, num=s_max_len, axis=1)\n",
    "        lstm_cell = rnn.BasicLSTMCell(s_hidden, reuse=reuse)\n",
    "        outputs, _ = rnn.static_rnn(lstm_cell, s_input, dtype=tf.float32, scope= scope)\n",
    "        # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "        outputs = tf.stack(outputs)\n",
    "        # and change back dimension to [batch_size(64), s_max_len(12), s_hidden(32)]\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        # Start indices for each sample\n",
    "        index = tf.range(0, batch_size) * (s_max_len) + (s_real_len-1)\n",
    "        # Indexing\n",
    "        outputs = tf.gather(tf.reshape(outputs, [-1, s_hidden]), index)\n",
    "        return outputs\n",
    "    \n",
    "    \"\"\"\n",
    "    Args\n",
    "        c: list of sentences, shape = [batch_size, 20, 12]\n",
    "        l: list of labels, shape = [batch_size, 20, 20]\n",
    "        c_real_len: list of real length, shape = [batch_size, 20]\n",
    "        \n",
    "    Returns\n",
    "        tagged_c_objects: list of embedded sentence + label, shape = [batch_size, 52] 20개\n",
    "        len(tagged_c_objects) = 20\n",
    "    \"\"\"\n",
    "    context = tf.unstack(c, axis=1) \n",
    "    real_lens = tf.unstack(c_real_len, axis=1)\n",
    "    labels = tf.unstack(l, axis=1)\n",
    "    tagged_c_objects = []\n",
    "    for sentence, real_len, label in zip(context, real_lens, labels):\n",
    "        s_embedded = sentenceLSTM(sentence, real_len, reuse=reuse)\n",
    "        c_embedded = tf.concat([s_embedded, label], axis= 1)\n",
    "        tagged_c_objects.append(c_embedded)\n",
    "                                 \n",
    "    return tagged_c_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def questionLSTM(q, q_real_len, q_hidden=q_hidden, reuse=True, scope = \"questionLSTM\"):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        q: zero padded qeustions, shape=[batch_size, q_max_len]\n",
    "        q_real_len: original question length, shape = [batch_size, 1]\n",
    "        \n",
    "    Returns\n",
    "        embedded_q: embedded questions, shape = [batch_size, q_hidden(32)]\n",
    "    \"\"\"\n",
    "    embedded_q_word = tf.nn.embedding_lookup(q_word_embed_matrix, q)\n",
    "    q_input = tf.unstack(embedded_q_word, num=q_max_len, axis=1)\n",
    "    lstm_cell = rnn.BasicLSTMCell(q_hidden, reuse=reuse)\n",
    "    outputs, _ = rnn.static_rnn(lstm_cell, q_input, dtype=tf.float32, scope = scope)\n",
    "    # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "    outputs = tf.stack(outputs)\n",
    "    # and change back dimension to [batch_size(64), q_max_len(12), s_hidden(32)]\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    # Start indices for each sample\n",
    "    index = tf.range(0, batch_size) * (q_max_len) + (q_real_len-1)\n",
    "    # Indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, s_hidden]), index)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_RN_input(embedded_c, embedded_q):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args\n",
    "        embedded_c: output of contextLSTM, 20 length list of embedded sentences\n",
    "        embedded_q: output of questionLSTM, embedded question\n",
    "        \n",
    "    Returns\n",
    "        RN_input: input for RN g_theta, shape = [batch_size*190, (52+52+32)]\n",
    "        considered batch_size and all combinations\n",
    "    \"\"\"\n",
    "    # 20 combination 2 --> total 190 object pairs\n",
    "    object_pairs = list(itertools.combinations(embedded_c, 2))\n",
    "    # concatenate with question\n",
    "    RN_inputs = []\n",
    "    for object_pair in object_pairs:\n",
    "        RN_input = tf.concat([object_pair[0], object_pair[1], embedded_q], axis=1)\n",
    "        RN_inputs.append(RN_input)\n",
    "    \n",
    "    return tf.concat(RN_inputs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RN\n",
    "* $g_\\theta$: 4 layer, all 256 units MLP, ReLU\n",
    "* $f_\\phi$: 3 layer, 256/512/159 units MLP, ReLU, sotmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_units = [256,256,256,256]\n",
    "f_units = [256,512,159]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fc(inputs, output_shape, activation_fcn = tf.nn.relu, name=\"fc\"):\n",
    "    output = slim.fully_connected(inputs, int(output_shape), activation_fn=activation_fcn)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm_relu(inputs, output_shape, phase=True, scope=None, activation= True):\n",
    "    with tf.variable_scope(scope):\n",
    "        h1 = fully_connected(inputs, output_shape, activation_fn=None, scope='dense')\n",
    "        h2 = batch_norm(h1, center=True, scale=True, is_training=phase, scope='bn')\n",
    "        if activation:\n",
    "            o = tf.nn.relu(h2, 'relu')\n",
    "        else:\n",
    "            o = h2\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def g_theta(RN_input, scope= 'g_theta', reuse= True): \n",
    "    \"\"\"\n",
    "    Args\n",
    "        RN_input: [o_i, o_j, q], shape = [batch_size*190, 136]\n",
    "        \n",
    "    Returns\n",
    "        g_output: shape = [190, batch_size, 256]\n",
    "    \"\"\"\n",
    "    input_dim = RN_input.shape[1]\n",
    "    with tf.variable_scope(scope, reuse= reuse) as scope:\n",
    "        # if not reuse: log.warn(scope.name): reuse하는지 확인하기 위한 출력옵션\n",
    "#         if not reuse:\n",
    "#             print(scope.name)\n",
    "        g_1 = batch_norm_relu(RN_input, g_units[0], scope= \"g_1\")\n",
    "        g_2 = batch_norm_relu(g_1, g_units[1], scope= \"g_2\")\n",
    "        g_3 = batch_norm_relu(g_2, g_units[2], scope= \"g_3\")\n",
    "        g_4 = batch_norm_relu(g_3, g_units[3], scope= \"g_4\")\n",
    "    g_output = tf.reshape(g_4, shape=[190, batch_size, g_units[3]])\n",
    "    return g_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f_phi(g, scope= 'f_phi', reuse=True):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        g: g_theta result, shape = [190, batch_size, 256]\n",
    "        \n",
    "    Returns\n",
    "        f_output: shape = [batch_size, 159]\n",
    "    \"\"\"\n",
    "    f_input = tf.reduce_sum(g, axis=0)\n",
    "    with tf.variable_scope(scope, reuse=reuse) as scope:\n",
    "        f_1 = batch_norm_relu(f_input, f_units[0], scope= \"f_1\")\n",
    "        f_2 = batch_norm_relu(f_1, f_units[1], scope= \"f_2\")\n",
    "        f_3 = batch_norm_relu(f_2, f_units[2], activation= None, scope= \"f_3\")\n",
    "    return f_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reuse..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(c, q, l, c_real_len, q_real_len):\n",
    "    embedded_c = contextLSTM(c, l, c_real_len, reuse=True)\n",
    "    embedded_q = questionLSTM(q, q_real_len, reuse=None)\n",
    "    RN_input = convert_to_RN_input(embedded_c, embedded_q)\n",
    "    f_input = g_theta(RN_input, reuse=None)\n",
    "    prediction = f_phi(f_input, reuse=None)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = model(c,q,l,c_real_len, q_real_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(125, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_1:0' shape=(89, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'sentenceLSTM/basic_lstm_cell/weights:0' shape=(64, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'sentenceLSTM/basic_lstm_cell/biases:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'questionLSTM/basic_lstm_cell/weights:0' shape=(64, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'questionLSTM/basic_lstm_cell/biases:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/g_1/dense/weights:0' shape=(136, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/g_1/dense/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/g_1/bn/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/g_1/bn/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/g_2/dense/weights:0' shape=(256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/g_2/dense/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/g_2/bn/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/g_2/bn/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/g_3/dense/weights:0' shape=(256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/g_3/dense/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/g_3/bn/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/g_3/bn/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/g_4/dense/weights:0' shape=(256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/g_4/dense/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/g_4/bn/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/g_4/bn/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/f_1/dense/weights:0' shape=(256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/f_1/dense/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/f_1/bn/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/f_1/bn/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/f_2/dense/weights:0' shape=(256, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/f_2/dense/biases:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/f_2/bn/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/f_2/bn/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/f_3/dense/weights:0' shape=(512, 159) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/f_3/dense/biases:0' shape=(159,) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/f_3/bn/beta:0' shape=(159,) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/f_3/bn/gamma:0' shape=(159,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "* 64 mini batches\n",
    "* cross-entropy loss function\n",
    "* Adam optimizer\n",
    "* learning rate: 2e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues\n",
    "1. multiple answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = tf.equal(tf.argmax(prediction, axis=1), tf.argmax(a, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimizer = opt.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_iter(c, q, l, a, c_real_len, q_real_len, batch_size=batch_size, num_epochs=iter_time, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    c = np.array(c)\n",
    "    q = np.array(q)\n",
    "    l = np.array(l)\n",
    "    a = np.array(a)\n",
    "    c_real_len = np.array(c_real_len)\n",
    "    q_real_len = np.array(q_real_len)\n",
    "    data_size = len(q)\n",
    "    num_batches_per_epoch = int(data_size/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"In epoch >> \" + str(epoch + 1))\n",
    "        print(\"num batches per epoch is: \" + str(num_batches_per_epoch))\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            c_shuffled = c[shuffle_indices]\n",
    "            q_shuffled = q[shuffle_indices]\n",
    "            l_shuffled = l[shuffle_indices]\n",
    "            a_shuffled = a[shuffle_indices]\n",
    "            c_real_len_shuffled = c_real_len[shuffle_indices]\n",
    "            q_real_len_shuffled = q_real_len[shuffle_indices]\n",
    "        else:\n",
    "            c_shuffled = c\n",
    "            q_shuffled = q\n",
    "            l_shuffled = l\n",
    "            a_shuffled = a\n",
    "            c_real_len_shuffled = c_real_len\n",
    "            q_real_len_shuffled = q_real_len\n",
    "        \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = (batch_num + 1) * batch_size\n",
    "            if end_index < data_size:\n",
    "                c_batch, q_batch, l_batch, a_batch, c_real_len_batch, q_real_len_batch = c_shuffled[start_index:end_index], q_shuffled[start_index:end_index], l_shuffled[start_index:end_index], a_shuffled[start_index:end_index], c_real_len_shuffled[start_index:end_index], q_real_len_shuffled[start_index:end_index]\n",
    "            yield list(zip(c_batch, q_batch, l_batch, a_batch, c_real_len_batch, q_real_len_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====training====\n",
      "In epoch >> 1\n",
      "num batches per epoch is: 2812\n",
      "step: 0\n",
      "====validation start====\n",
      "In epoch >> 1\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 2\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 3\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 4\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 5\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 6\n",
      "num batches per epoch is: 312\n",
      "Mean accuracy=0.0100410657051\n",
      "====training====\n",
      "step: 100\n",
      "====validation start====\n",
      "In epoch >> 1\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 2\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 3\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 4\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 5\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 6\n",
      "num batches per epoch is: 312\n",
      "Mean accuracy=0.308476896368\n",
      "====training====\n",
      "step: 200\n",
      "====validation start====\n",
      "In epoch >> 1\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 2\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 3\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 4\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 5\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 6\n",
      "num batches per epoch is: 312\n",
      "Mean accuracy=0.405991252671\n",
      "====training====\n",
      "step: 300\n",
      "====validation start====\n",
      "In epoch >> 1\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 2\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 3\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 4\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 5\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 6\n",
      "num batches per epoch is: 312\n",
      "Mean accuracy=0.465494791667\n",
      "====training====\n",
      "step: 400\n",
      "====validation start====\n",
      "In epoch >> 1\n",
      "num batches per epoch is: 312\n",
      "In epoch >> 2\n",
      "num batches per epoch is: 312\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    print(\"====training====\")    \n",
    "    batch_train = batch_iter(train_c, train_q, train_l, train_a, train_c_real_len, train_q_real_len)\n",
    "    for train in batch_train:\n",
    "        c_batch, q_batch, l_batch, a_batch, c_real_len_batch, q_real_len_batch = zip(*train)\n",
    "        current_step = sess.run(global_step, feed_dict={c:c_batch, q:q_batch, l:l_batch, a:a_batch, c_real_len:c_real_len_batch, q_real_len:q_real_len_batch})\n",
    "        optimizer.run(feed_dict={c:c_batch, q:q_batch, l:l_batch, a:a_batch, c_real_len:c_real_len_batch, q_real_len:q_real_len_batch})\n",
    "        if current_step % (display_step) == 0:\n",
    "            print(\"step: {}\".format(current_step))\n",
    "            print(\"====validation start====\")\n",
    "            batch_val = batch_iter(val_c, val_q, val_l, val_a, val_c_real_len, val_q_real_len)\n",
    "            accs = []\n",
    "            for val in batch_val:\n",
    "                c_val, q_val, l_val, a_val, c_real_len_val, q_real_len_val = zip(*val)\n",
    "                acc = accuracy.eval(feed_dict={c:c_val, q:q_val, l:l_val, a:a_val, c_real_len:c_real_len_val, q_real_len:q_real_len_val})\n",
    "                accs.append(acc)\n",
    "            print(\"Mean accuracy=\" + str(sum(accs)/len(accs)))\n",
    "            print(\"====training====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
