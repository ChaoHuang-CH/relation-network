{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 용어\n",
    "context: question에 대한 hint sentence의 묶음 <br>\n",
    "sentence: context를 이루고 있는 문장 하나 <br>\n",
    "question: 질문 (문장 하나)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import slim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# question, answer, context, label\n",
    "with open('./babi_preprocessd/train_dataset_masked.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open('./babi_preprocessd/val_dataset_masked.pkl', 'rb') as f:\n",
    "    val = pickle.load(f)\n",
    "with open('./babi_preprocessd/test_dataset_masked.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./babi_preprocessd/c_word_set.pkl', 'rb') as f:\n",
    "    c_word_set = pickle.load(f)\n",
    "with open('./babi_preprocessd/q_word_set.pkl', 'rb') as f:\n",
    "    q_word_set = pickle.load(f)\n",
    "with open('./babi_preprocessd/a_word_set.pkl', 'rb') as f:\n",
    "    a_word_set = pickle.load(f)\n",
    "with open('./babi_preprocessd/cqa_word_set.pkl', 'rb') as f:\n",
    "    cqa_word_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[train_q, train_a, train_c, train_l, train_c_real_len, train_q_real_len] = train\n",
    "[val_q, val_a, val_c, val_l, val_c_real_len, val_q_real_len] = val\n",
    "[test_q, test_a, test_c, test_l, test_c_real_len, test_q_real_len] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_max_len = 20\n",
    "s_max_len = 12\n",
    "q_max_len = 12\n",
    "mask_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 2e-4\n",
    "batch_size = 64\n",
    "iter_time = 6\n",
    "display_step = 10\n",
    "seed = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* context words: 124 <br>\n",
    "* question words: 88 <br>\n",
    "* answer words: 41 <br>\n",
    "* all words: 159 <br>\n",
    "* s,q max len: 12 <br>\n",
    "* c max len: 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM for context\n",
    "* 32 unit LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM for question\n",
    "* 32 unit LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model parameter\n",
    "s_input_step = s_max_len\n",
    "s_hidden = 32\n",
    "# ---\n",
    "q_input_step = q_max_len\n",
    "q_hidden = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# embed matrix\n",
    "c_word_embed = 52\n",
    "c_vocab_size = len(c_word_set)+1 # consider masking\n",
    "c_word_embed_matrix = tf.Variable(tf.random_uniform(shape=[c_vocab_size, c_word_embed], minval=-1, maxval=1, seed= seed))\n",
    "# ---\n",
    "q_word_embed = 52\n",
    "q_vocab_size = len(q_word_set)+1 # consider masking\n",
    "q_word_embed_matrix = tf.Variable(tf.random_uniform(shape=[q_vocab_size, q_word_embed], minval=-1, maxval=1, seed=seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input, output\n",
    "c = tf.placeholder(dtype=tf.int32, shape=[batch_size, c_max_len, s_max_len])\n",
    "c_real_len = tf.placeholder(dtype=tf.int32, shape=[batch_size, c_max_len])\n",
    "s = tf.placeholder(dtype=tf.int32, shape=[batch_size, s_max_len]) \n",
    "q = tf.placeholder(dtype=tf.int32, shape=[batch_size, q_max_len])\n",
    "q_real_len = tf.placeholder(dtype=tf.int32, shape=[batch_size])\n",
    "l = tf.placeholder(dtype=tf.float32, shape=[batch_size, c_max_len, c_max_len])\n",
    "a = tf.placeholder(dtype=tf.float32, shape=[batch_size, len(cqa_word_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contextLSTM(c, l, c_real_len, reuse=True, scope= \"contextLSTM\"):\n",
    "    \n",
    "    def sentenceLSTM(s, s_real_len, s_hidden=s_hidden, s_max_len= s_max_len, reuse=reuse, scope= \"sentenceLSTM\"):\n",
    "        \"\"\"\n",
    "        embedding sentence\n",
    "\n",
    "        Arguments\n",
    "            s: sentence (word index list), shape = [batch_size, 12]\n",
    "            s_real_len: length of the sentence before zero padding, int32\n",
    "\n",
    "        Returns\n",
    "            embedded_s: embedded sentence, shape = [batch_size, 32]\n",
    "        \"\"\"\n",
    "        embedded_sent_word = tf.nn.embedding_lookup(c_word_embed_matrix, s)\n",
    "        s_input = tf.unstack(embedded_sent_word, num=s_max_len, axis=1)\n",
    "        lstm_cell = rnn.BasicLSTMCell(s_hidden, reuse=reuse)\n",
    "        outputs, _ = rnn.static_rnn(lstm_cell, s_input, dtype=tf.float32, scope= scope)\n",
    "        # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "        outputs = tf.stack(outputs)\n",
    "        # and change back dimension to [batch_size(64), s_max_len(12), s_hidden(32)]\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        # Start indices for each sample\n",
    "        index = tf.range(0, batch_size) * (s_max_len) + (s_real_len-1)\n",
    "        # Indexing\n",
    "        outputs = tf.gather(tf.reshape(outputs, [-1, s_hidden]), index)\n",
    "        return outputs\n",
    "    \n",
    "    \"\"\"\n",
    "    Args\n",
    "        c: list of sentences, shape = [batch_size, 20, 12]\n",
    "        l: list of labels, shape = [batch_size, 20, 20]\n",
    "        c_real_len: list of real length, shape = [batch_size, 20]\n",
    "        \n",
    "    Returns\n",
    "        tagged_c_objects: list of embedded sentence + label, shape = [batch_size, 52] 20개\n",
    "        len(tagged_c_objects) = 20\n",
    "    \"\"\"\n",
    "    context = tf.unstack(c, axis=1) \n",
    "    real_lens = tf.unstack(c_real_len, axis=1)\n",
    "    labels = tf.unstack(l, axis=1)\n",
    "    tagged_c_objects = []\n",
    "    for sentence, real_len, label in zip(context, real_lens, labels):\n",
    "        s_embedded = sentenceLSTM(sentence, real_len, reuse=reuse)\n",
    "        c_embedded = tf.concat([s_embedded, label], axis= 1)\n",
    "        tagged_c_objects.append(c_embedded)\n",
    "                                 \n",
    "    return tagged_c_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def questionLSTM(q, q_real_len, q_hidden=q_hidden, reuse=True, scope = \"questionLSTM\"):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        q: zero padded qeustions, shape=[batch_size, q_max_len]\n",
    "        q_real_len: original question length, shape = [batch_size, 1]\n",
    "        \n",
    "    Returns\n",
    "        embedded_q: embedded questions, shape = [batch_size, q_hidden(32)]\n",
    "    \"\"\"\n",
    "    embedded_q_word = tf.nn.embedding_lookup(q_word_embed_matrix, q)\n",
    "    q_input = tf.unstack(embedded_q_word, num=q_max_len, axis=1)\n",
    "    lstm_cell = rnn.BasicLSTMCell(q_hidden, reuse=reuse)\n",
    "    outputs, _ = rnn.static_rnn(lstm_cell, q_input, dtype=tf.float32, scope = scope)\n",
    "    # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "    outputs = tf.stack(outputs)\n",
    "    # and change back dimension to [batch_size(64), q_max_len(12), s_hidden(32)]\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    # Start indices for each sample\n",
    "    index = tf.range(0, batch_size) * (q_max_len) + (q_real_len-1)\n",
    "    # Indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, s_hidden]), index)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_RN_input(embedded_c, embedded_q):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args\n",
    "        embedded_c: output of contextLSTM, 20 length list of embedded sentences\n",
    "        embedded_q: output of questionLSTM, embedded question\n",
    "        \n",
    "    Returns\n",
    "        RN_input: input for RN g_theta, shape = [batch_size*190, (52+52+32)]\n",
    "        considered batch_size and all combinations\n",
    "    \"\"\"\n",
    "    # 20 combination 2 --> total 190 object pairs\n",
    "    object_pairs = list(itertools.combinations(embedded_c, 2))\n",
    "    # concatenate with question\n",
    "    RN_inputs = []\n",
    "    for object_pair in object_pairs:\n",
    "        RN_input = tf.concat([object_pair[0], object_pair[1], embedded_q], axis=1)\n",
    "        RN_inputs.append(RN_input)\n",
    "    \n",
    "    return tf.concat(RN_inputs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RN\n",
    "* $g_\\theta$: 4 layer, all 256 units MLP, ReLU\n",
    "* $f_\\phi$: 3 layer, 256/512/159 units MLP, ReLU, sotmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_units = [256,256,256,256]\n",
    "f_units = [256,512,159]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fc(inputs, output_shape, activation_fcn = tf.nn.relu, name=\"fc\"):\n",
    "    output = slim.fully_connected(inputs, int(output_shape), activation_fn=activation_fcn)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def g_theta(RN_input, scope= 'g_theta', reuse= True): \n",
    "    \"\"\"\n",
    "    Args\n",
    "        RN_input: [o_i, o_j, q], shape = [batch_size*190, 136]\n",
    "        \n",
    "    Returns\n",
    "        g_output: shape = [batch_size, 190, 256]\n",
    "    \"\"\"\n",
    "    input_dim = RN_input.shape[1]\n",
    "    with tf.variable_scope(scope, reuse= reuse) as scope:\n",
    "        # if not reuse: log.warn(scope.name): reuse하는지 확인하기 위한 출력옵션\n",
    "        if not reuse:\n",
    "            print(scope.name)\n",
    "        g_1 = fc(RN_input, g_units[0], name= \"g_1\")\n",
    "        g_2 = fc(g_1, g_units[1], name= \"g_2\")\n",
    "        g_3 = fc(g_2, g_units[2], name= \"g_3\")\n",
    "        g_4 = fc(g_3, g_units[3], name= \"g_4\")\n",
    "    g_output = tf.reshape(g_4, shape=[batch_size, 190, g_units[3]])\n",
    "    return g_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f_phi(g, scope= 'f_phi', reuse=True):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        g: g_theta result, shape = [batch_size, 190, 256]\n",
    "        \n",
    "    Returns\n",
    "        f_output: shape = [batch_size, 159]\n",
    "    \"\"\"\n",
    "    f_input = tf.reduce_sum(g, axis=1)\n",
    "    with tf.variable_scope(scope, reuse=reuse) as scope:\n",
    "        f_1 = fc(f_input, f_units[0], name= \"f_1\")\n",
    "        f_2 = fc(f_1, f_units[1], name= \"f_2\")\n",
    "        f_3 = fc(f_2, f_units[2], activation_fcn= None, name= \"f_3\")\n",
    "    return f_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reuse..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(c, q, l, c_real_len, q_real_len):\n",
    "    embedded_c = contextLSTM(c, l, c_real_len, reuse=True)\n",
    "    embedded_q = questionLSTM(q, q_real_len, reuse=None)\n",
    "    RN_input = convert_to_RN_input(embedded_c, embedded_q)\n",
    "    f_input = g_theta(RN_input, reuse=None)\n",
    "    prediction = f_phi(f_input, reuse=None)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_theta\n"
     ]
    }
   ],
   "source": [
    "prediction = model(c,q,l,c_real_len, q_real_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(125, 52) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_1:0' shape=(89, 52) dtype=float32_ref>,\n",
       " <tf.Variable 'sentenceLSTM/basic_lstm_cell/weights:0' shape=(84, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'sentenceLSTM/basic_lstm_cell/biases:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'questionLSTM/basic_lstm_cell/weights:0' shape=(84, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'questionLSTM/basic_lstm_cell/biases:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/fully_connected/weights:0' shape=(136, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/fully_connected/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/fully_connected_1/weights:0' shape=(256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/fully_connected_1/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/fully_connected_2/weights:0' shape=(256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/fully_connected_2/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/fully_connected_3/weights:0' shape=(256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'g_theta/fully_connected_3/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/fully_connected/weights:0' shape=(256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/fully_connected/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/fully_connected_1/weights:0' shape=(256, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/fully_connected_1/biases:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/fully_connected_2/weights:0' shape=(512, 159) dtype=float32_ref>,\n",
       " <tf.Variable 'f_phi/fully_connected_2/biases:0' shape=(159,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "* 64 mini batches\n",
    "* cross-entropy loss function\n",
    "* Adam optimizer\n",
    "* learning rate: 2e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues\n",
    "1. multiple answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = tf.equal(tf.argmax(prediction, axis=1), tf.argmax(a, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimizer = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_iter(c, q, l, a, c_real_len, q_real_len, batch_size=batch_size, num_epochs=iter_time, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    c = np.array(c)\n",
    "    q = np.array(q)\n",
    "    l = np.array(l)\n",
    "    a = np.array(a)\n",
    "    c_real_len = np.array(c_real_len)\n",
    "    q_real_len = np.array(q_real_len)\n",
    "    data_size = len(q)\n",
    "    num_batches_per_epoch = int(data_size/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"In epoch >> \" + str(epoch + 1))\n",
    "        print(\"num batches per epoch is: \" + str(num_batches_per_epoch))\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            c_shuffled = c[shuffle_indices]\n",
    "            q_shuffled = q[shuffle_indices]\n",
    "            l_shuffled = l[shuffle_indices]\n",
    "            a_shuffled = a[shuffle_indices]\n",
    "            c_real_len_shuffled = c_real_len[shuffle_indices]\n",
    "            q_real_len_shuffled = q_real_len[shuffle_indices]\n",
    "        else:\n",
    "            c_shuffled = c\n",
    "            q_shuffled = q\n",
    "            l_shuffled = l\n",
    "            a_shuffled = a\n",
    "            c_real_len_shuffled = c_real_len\n",
    "            q_real_len_shuffled = q_real_len\n",
    "        \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = (batch_num + 1) * batch_size\n",
    "            if end_index < data_size:\n",
    "                c_batch, q_batch, l_batch, a_batch, c_real_len_batch, q_real_len_batch = c_shuffled[start_index:end_index], q_shuffled[start_index:end_index], l_shuffled[start_index:end_index], a_shuffled[start_index:end_index], c_real_len_shuffled[start_index:end_index], q_real_len_shuffled[start_index:end_index]\n",
    "            yield list(zip(c_batch, q_batch, l_batch, a_batch, c_real_len_batch, q_real_len_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch >> 1\n",
      "num batches per epoch is: 2812\n",
      "0.0625\n",
      "0.109375\n",
      "0.125\n",
      "0.09375\n",
      "0.109375\n",
      "0.046875\n",
      "0.140625\n",
      "0.15625\n",
      "0.09375\n",
      "0.140625\n",
      "0.140625\n",
      "0.078125\n",
      "0.0625\n",
      "0.078125\n",
      "0.171875\n",
      "0.140625\n",
      "0.09375\n",
      "0.109375\n",
      "0.09375\n",
      "0.234375\n",
      "0.0625\n",
      "0.1875\n",
      "0.0625\n",
      "0.109375\n",
      "0.125\n",
      "0.109375\n",
      "0.078125\n",
      "0.03125\n",
      "0.078125\n",
      "0.171875\n",
      "0.109375\n",
      "0.171875\n",
      "0.203125\n",
      "0.21875\n",
      "0.203125\n",
      "0.09375\n",
      "0.09375\n",
      "0.09375\n",
      "0.09375\n",
      "0.046875\n",
      "0.15625\n",
      "0.109375\n",
      "0.140625\n",
      "0.09375\n",
      "0.0625\n",
      "0.078125\n",
      "0.125\n",
      "0.203125\n",
      "0.0625\n",
      "0.15625\n",
      "0.078125\n",
      "0.125\n",
      "0.09375\n",
      "0.03125\n",
      "0.09375\n",
      "0.046875\n",
      "0.171875\n",
      "0.09375\n",
      "0.125\n",
      "0.09375\n",
      "0.125\n",
      "0.09375\n",
      "0.140625\n",
      "0.109375\n",
      "0.125\n",
      "0.125\n",
      "0.15625\n",
      "0.125\n",
      "0.125\n",
      "0.140625\n",
      "0.140625\n",
      "0.078125\n",
      "0.171875\n",
      "0.125\n",
      "0.078125\n",
      "0.078125\n",
      "0.109375\n",
      "0.171875\n",
      "0.0625\n",
      "0.03125\n",
      "0.125\n",
      "0.09375\n",
      "0.078125\n",
      "0.078125\n",
      "0.125\n",
      "0.109375\n",
      "0.046875\n",
      "0.09375\n",
      "0.125\n",
      "0.046875\n",
      "0.15625\n",
      "0.046875\n",
      "0.15625\n",
      "0.0625\n",
      "0.125\n",
      "0.0\n",
      "0.09375\n",
      "0.125\n",
      "0.09375\n",
      "0.171875\n",
      "0.015625\n",
      "0.109375\n",
      "0.078125\n",
      "0.03125\n",
      "0.015625\n",
      "0.140625\n",
      "0.125\n",
      "0.078125\n",
      "0.0625\n",
      "0.078125\n",
      "0.03125\n",
      "0.046875\n",
      "0.0625\n",
      "0.015625\n",
      "0.09375\n",
      "0.109375\n",
      "0.109375\n",
      "0.015625\n",
      "0.015625\n",
      "0.046875\n",
      "0.046875\n",
      "0.109375\n",
      "0.046875\n",
      "0.0\n",
      "0.03125\n",
      "0.015625\n",
      "0.015625\n",
      "0.15625\n",
      "0.125\n",
      "0.078125\n",
      "0.046875\n",
      "0.0625\n",
      "0.015625\n",
      "0.0\n",
      "0.046875\n",
      "0.109375\n",
      "0.046875\n",
      "0.03125\n",
      "0.046875\n",
      "0.015625\n",
      "0.109375\n",
      "0.140625\n",
      "0.0\n",
      "0.0625\n",
      "0.03125\n",
      "0.109375\n",
      "0.0625\n",
      "0.015625\n",
      "0.15625\n",
      "0.09375\n",
      "0.046875\n",
      "0.046875\n",
      "0.171875\n",
      "0.046875\n",
      "0.015625\n",
      "0.015625\n",
      "0.03125\n",
      "0.0625\n",
      "0.140625\n",
      "0.140625\n",
      "0.046875\n",
      "0.09375\n",
      "0.125\n",
      "0.09375\n",
      "0.0625\n",
      "0.0\n",
      "0.09375\n",
      "0.0\n",
      "0.0\n",
      "0.015625\n",
      "0.0\n",
      "0.015625\n",
      "0.09375\n",
      "0.0\n",
      "0.03125\n",
      "0.0625\n",
      "0.109375\n",
      "0.046875\n",
      "0.03125\n",
      "0.078125\n",
      "0.125\n",
      "0.0\n",
      "0.046875\n",
      "0.015625\n",
      "0.109375\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.03125\n",
      "0.171875\n",
      "0.0625\n",
      "0.140625\n",
      "0.03125\n",
      "0.03125\n",
      "0.0\n",
      "0.015625\n",
      "0.03125\n",
      "0.140625\n",
      "0.140625\n",
      "0.0\n",
      "0.0625\n",
      "0.03125\n",
      "0.015625\n",
      "0.0625\n",
      "0.0\n",
      "0.109375\n",
      "0.0\n",
      "0.046875\n",
      "0.09375\n",
      "0.09375\n",
      "0.078125\n",
      "0.0\n",
      "0.015625\n",
      "0.03125\n",
      "0.03125\n",
      "0.03125\n",
      "0.015625\n",
      "0.0\n",
      "0.125\n",
      "0.03125\n",
      "0.09375\n",
      "0.171875\n",
      "0.078125\n",
      "0.09375\n",
      "0.0\n",
      "0.078125\n",
      "0.109375\n",
      "0.25\n",
      "0.109375\n",
      "0.015625\n",
      "0.0625\n",
      "0.0\n",
      "0.0\n",
      "0.0625\n",
      "0.046875\n",
      "0.125\n",
      "0.0625\n",
      "0.015625\n",
      "0.015625\n",
      "0.125\n",
      "0.15625\n",
      "0.015625\n",
      "0.0\n",
      "0.046875\n",
      "0.109375\n",
      "0.015625\n",
      "0.015625\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.015625\n",
      "0.03125\n",
      "0.015625\n",
      "0.09375\n",
      "0.15625\n",
      "0.078125\n",
      "0.0\n",
      "0.03125\n",
      "0.078125\n",
      "0.0\n",
      "0.109375\n",
      "0.078125\n",
      "0.0625\n",
      "0.109375\n",
      "0.03125\n",
      "0.046875\n",
      "0.0\n",
      "0.015625\n",
      "0.21875\n",
      "0.046875\n",
      "0.078125\n",
      "0.0\n",
      "0.15625\n",
      "0.0625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-68aa908346c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mc_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_real_len_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_real_len_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mc_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mq_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ml_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ma_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_real_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mc_real_len_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_real_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mq_real_len_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mc_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mq_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ml_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ma_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_real_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mc_real_len_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_real_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mq_real_len_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jihyung/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \"\"\"\n\u001b[0;32m-> 1552\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jihyung/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3774\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3775\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3776\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jihyung/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 786\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    787\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jihyung/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 994\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    995\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jihyung/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1044\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1045\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/jihyung/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1049\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jihyung/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1032\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    batch_train = batch_iter(train_c, train_q, train_l, train_a, train_c_real_len, train_q_real_len)\n",
    "    for train in batch_train:\n",
    "        c_batch, q_batch, l_batch, a_batch, c_real_len_batch, q_real_len_batch = zip(*train)\n",
    "        optimizer.run(feed_dict={c:c_batch, q:q_batch, l:l_batch, a:a_batch, c_real_len:c_real_len_batch, q_real_len:q_real_len_batch})\n",
    "        result = accuracy.eval(feed_dict={c:c_batch, q:q_batch, l:l_batch, a:a_batch, c_real_len:c_real_len_batch, q_real_len:q_real_len_batch})\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array dimension check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = np.array([i for i in range(24)])\n",
    "test_1 = test.reshape([2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.concatenate(test_1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_2 = test_1.reshape([-1,4])\n",
    "test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_2.reshape([2,3,4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
